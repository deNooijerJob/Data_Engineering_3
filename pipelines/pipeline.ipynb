{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "import pandas as pd\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host='https://7213e46a0ed7fcf9-dot-us-central2.pipelines.googleusercontent.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('gs://sentiment_analysis_de2020/data/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', header=0, names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "# sample = data.sample(1000, random_state=1)\n",
    "# filename = 'test_data.csv'\n",
    "# sample.to_csv(filename)\n",
    "# client = storage.Client()\n",
    "# bucket = client.get_bucket('sentiment_analysis_de2020')  \n",
    "# blob = bucket.blob('data/' + filename)\n",
    "# blob.upload_from_filename(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('gs://sentiment_analysis_de2020/data/preprocessed_test_data.csv')\n",
    "data.text.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_path:str, encoder:str, bucket_name: str, stem: bool, test:bool) -> str:\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from  nltk.stem import SnowballStemmer\n",
    "    import re\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from google.cloud import storage\n",
    "\n",
    "    #data = pd.read_csv(data_path, encoding=encoder, header=0, names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "    data = pd.read_csv(data_path)\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = stopwords.words('english')\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    for i in range(0, len(data.text)):\n",
    "        text = data.text[i]\n",
    "        text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", ' ', str(text).lower()).strip()\n",
    "        tokens = []\n",
    "        for token in text.split():\n",
    "            if token not in stop_words:\n",
    "                if stem:\n",
    "                    tokens.append(stemmer.stem(token))\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "        temp = \" \".join(tokens)\n",
    "        data.text[i] = temp\n",
    "    \n",
    "    \n",
    "    data.drop(data.columns[:1], axis=1, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.dropna()\n",
    "    \n",
    "    if not test:\n",
    "        filename = 'preprocessed_data.csv'\n",
    "    else: \n",
    "        filename = 'preprocessed_test_data.csv'\n",
    "    data.to_csv(filename)\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)  \n",
    "    blob = bucket.blob('data/' + filename)\n",
    "    blob.upload_from_filename(filename)\n",
    "    \n",
    "    \n",
    "    preprocessed_data_path = ('gs://sentiment_analysis_de2020/data/' + filename)\n",
    "    return preprocessed_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_op = comp.create_component_from_func(\n",
    "    func=preprocess,\n",
    "    base_image='python:3.7',\n",
    "    output_component_file='download_data.yaml', \n",
    "    packages_to_install=['pandas', 'fsspec', 'gcsfs', 'nltk', 'gensim', 'h5py==2.10.0', 'numpy==1.16.0','keras', 'tensorflow', 'google-cloud-storage']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bucket_name:str) -> str:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from  nltk.stem import SnowballStemmer\n",
    "    import re\n",
    "    import json\n",
    "    import pickle\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "    from keras import utils\n",
    "    from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    client = storage.Client() #setup the storage \n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    \n",
    "    blob_w2v_model = bucket.blob('models/model.w2v')\n",
    "    blob_w2v_model.download_to_filename('downloaded_model.w2v')\n",
    "    model_w2v = pickle.load(open(\"downloaded_model.w2v\", 'rb'))\n",
    "    \n",
    "    blob_tokenizer = bucket.blob('models/tokenizer.pkl')\n",
    "    blob_tokenizer.download_to_filename('downloaded_tokenizer.pkl')\n",
    "    tokenizer = pickle.load(open(\"downloaded_tokenizer.pkl\", 'rb'))\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    \n",
    "    for word, i in tokenizer.word_index.items():\n",
    "         if word in model_w2v.wv:\n",
    "            embedding_matrix[i] = model_w2v.wv[word]\n",
    "    embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "            optimizer=\"adam\",\n",
    "            metrics=['accuracy'])\n",
    "        \n",
    "    callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n",
    "    \n",
    "    model.save('model.h5')\n",
    "    upload_blob = bucket.blob('models/model.h5')\n",
    "    upload_blob.upload_from_filename('model.h5')    \n",
    "        \n",
    "    return 'models/model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model_op = comp.create_component_from_func(\n",
    "    func=build_model,\n",
    "    base_image='python:3.7',\n",
    "    output_component_file='build_model.yaml', \n",
    "    packages_to_install=['pandas', 'fsspec', 'gcsfs', 'nltk', 'gensim', 'h5py==2.10.0', 'numpy==1.16.0','keras', 'tensorflow', 'google-cloud-storage']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_path:str, model_path:str, bucket_name:str, num_epochs:int) -> str:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from  nltk.stem import SnowballStemmer\n",
    "    import re\n",
    "    import json\n",
    "    import pickle\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "    from keras import utils\n",
    "    from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "    from google.cloud import storage\n",
    "    from keras.models import load_model\n",
    "\n",
    "    client = storage.Client() #setup the storage \n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob_model = bucket.blob(model_path) # get the models from the bucket\n",
    "    blob_model.download_to_filename('downloaded_model.h5') # download the model\n",
    "    model = load_model('downloaded_model.h5') #load the model\n",
    "\n",
    "    blob_tokenizer = bucket.blob('models/tokenizer.pkl')\n",
    "    blob_tokenizer.download_to_filename('downloaded_tokenizer.pkl')\n",
    "    tokenizer = pickle.load(open(\"downloaded_tokenizer.pkl\", 'rb'))\n",
    "    \n",
    "    data = pd.read_csv(data_path)\n",
    "    X_train = pad_sequences(tokenizer.texts_to_sequences(data.text.astype(str)), maxlen=300)\n",
    "    y_train = data.target\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                epochs=num_epochs, \n",
    "                batch_size=32)\n",
    "    model.save('model.h5')\n",
    "    upload_blob = bucket.blob('models/model.h5')\n",
    "    upload_blob.upload_from_filename('model.h5')    \n",
    "        \n",
    "    return 'models/model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = comp.create_component_from_func(\n",
    "    func=train,\n",
    "    base_image='python:3.7',\n",
    "    output_component_file='train_model.yaml', \n",
    "    packages_to_install=['pandas', 'fsspec', 'gcsfs', 'nltk', 'gensim', 'h5py==2.10.0', 'numpy==1.16.0','keras', 'tensorflow', 'google-cloud-storage']\n",
    ")\n",
    "#train_op = comp.load_component_from_file('train_model.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(data_path:str, bucket_name:str, num_epochs: int) -> str: \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from  nltk.stem import SnowballStemmer\n",
    "    import re\n",
    "    import json\n",
    "    import pickle\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.models import Sequential\n",
    "    from keras.models import load_model\n",
    "    from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "    from keras import utils\n",
    "    from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "    from google.cloud import storage\n",
    " \n",
    "    client = storage.Client() #setup the storage \n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob_model = bucket.blob('models/model.h5') # get the models from the bucket\n",
    "    blob_model.download_to_filename('downloaded_model.h5') # download the model\n",
    "    model = load_model('downloaded_model.h5') #load the model\n",
    "\n",
    "    blob_tokenizer = bucket.blob('models/tokenizer.pkl')\n",
    "    blob_tokenizer.download_to_filename('downloaded_tokenizer.pkl')\n",
    "    tokenizer = pickle.load(open(\"downloaded_tokenizer.pkl\", 'rb'))\n",
    "    \n",
    "    data = pd.read_csv(data_path)\n",
    "    X_train = pad_sequences(tokenizer.texts_to_sequences(data.text.astype(str)), maxlen=300)\n",
    "    y_train = data.target\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "                epochs=num_epochs, \n",
    "                batch_size=100)\n",
    "    model.save('model.h5')\n",
    "    upload_blob = bucket.blob('models/model.h5')\n",
    "    upload_blob.upload_from_filename('model.h5')    \n",
    "        \n",
    "    return 'models/model.h5'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_op = comp.create_component_from_func(\n",
    "     func=retrain,\n",
    "     base_image='python:3.7',\n",
    "     output_component_file='retrain_model.yaml', \n",
    "     packages_to_install=['pandas', 'fsspec', 'gcsfs', 'nltk', 'gensim', 'h5py==2.10.0', 'numpy==1.16.0','keras', 'tensorflow', 'google-cloud-storage', 'sklearn']\n",
    ")\n",
    "#retrain_op = comp.load_component_from_file('retrain_model.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path:str, test_data_path:str, bucket_name:str) -> float:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from keras import utils\n",
    "    from keras.models import load_model\n",
    "    from google.cloud import storage\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    \n",
    "    client = storage.Client() #setup the storage \n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    \n",
    "    blob_tokenizer = bucket.blob('models/tokenizer.pkl')\n",
    "    blob_tokenizer.download_to_filename('downloaded_tokenizer.pkl')\n",
    "    tokenizer = pickle.load(open(\"downloaded_tokenizer.pkl\", 'rb'))\n",
    "    \n",
    "    data = pd.read_csv(test_data_path)\n",
    "    X_test = pad_sequences(tokenizer.texts_to_sequences(data.text.astype(str)), maxlen=300)\n",
    "    y_test = data.target\n",
    "    \n",
    "  \n",
    "    blob_model = bucket.blob(model_path) # get the models from the bucket\n",
    "    blob_model.download_to_filename('downloaded_model.h5') # download the model\n",
    "    model = load_model('downloaded_model.h5') #load the model\n",
    "    \n",
    "    predictions = model.predict(X_test,  verbose=1)\n",
    "    \n",
    "    for i in range(0, len(predictions)):\n",
    "        if (predictions[i] < 0.5):\n",
    "            predictions[i] = 0\n",
    "        elif (predictions[i] >= 0.5):\n",
    "            predictions[i] = 4\n",
    "        \n",
    "    print(\"confusion matrix : \\n\")\n",
    "    conf = confusion_matrix(y_test, predictions)\n",
    "    print(conf)\n",
    "    \n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    print('Accuracy: ' + str(acc))\n",
    "    \n",
    "    prec = precision_score(y_test, predictions, average='macro')\n",
    "    print('Precision: ' + str(prec))\n",
    "    \n",
    "    recall = recall_score(y_test, predictions, average='macro')\n",
    "    print('Recall: ' + str(acc))\n",
    "    \n",
    "    return(float((sum(predictions)/len(predictions))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_op = comp.create_component_from_func(\n",
    "     func=evaluate,\n",
    "     base_image='python:3.7',\n",
    "     output_component_file='evaluate.yaml', \n",
    "     packages_to_install=['pandas', 'fsspec', 'gcsfs', 'nltk', 'gensim', 'h5py==2.10.0', 'numpy==1.16.0','keras', 'tensorflow', 'google-cloud-storage', 'sklearn']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://7213e46a0ed7fcf9-dot-us-central2.pipelines.googleusercontent.com//#/experiments/details/bed88594-438b-4c43-a50f-ace7837fffab\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://7213e46a0ed7fcf9-dot-us-central2.pipelines.googleusercontent.com//#/runs/details/52c810e7-eef0-4ab4-bf34-1e9f1fc8d0ed\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=52c810e7-eef0-4ab4-bf34-1e9f1fc8d0ed)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name='twitter sentiment',\n",
    "    description='sentiment anlysis on live twitter data'\n",
    ")\n",
    "def sentiment_pipeline(data_path, encoder, bucket_name, stem, num_epochs, retrain, test_data_path, disable_cache):\n",
    "    \n",
    "    preprocess_task = preprocess_op(data_path, encoder, bucket_name, stem, False)\n",
    "    preprocess_test_task = preprocess_op(test_data_path, encoder, bucket_name, stem, True)\n",
    "    \n",
    "    with dsl.Condition(retrain == False):\n",
    "        build_model_task = build_model_op(bucket_name)\n",
    "        train_task = train_op(preprocess_task.output, build_model_task.output, bucket_name, num_epochs)\n",
    "        eval_task_train = evaluate_op(train_task.output, preprocess_test_task.output, bucket_name)\n",
    "        \n",
    "    with dsl.Condition(retrain == True):\n",
    "        retrain_task = retrain_op(preprocess_task.output, bucket_name, num_epochs)\n",
    "        eval_task_retrain = evaluate_op(retrain_task.output, preprocess_test_task.output, bucket_name)\n",
    "        \n",
    "    if disable_cache:\n",
    "      preprocess_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "      preprocess_test_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "      build_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "      train_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "      retrain_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "      eval_task_retrain.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "      eval_task_train.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "    \n",
    "arguments = {\n",
    "    'data_path': 'gs://sentiment_analysis_de2020/data/test_data.csv',\n",
    "    'encoder': 'ISO-8859-1',\n",
    "    'bucket_name': 'sentiment_analysis_de2020',\n",
    "    'stem': False, \n",
    "    'num_epochs': 4,\n",
    "    'retrain': True, \n",
    "    'test_data_path': 'gs://sentiment_analysis_de2020/data/sampled_data.csv', \n",
    "    'disable_cache': True\n",
    "}\n",
    "\n",
    "client.create_run_from_pipeline_func(sentiment_pipeline, arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
